# Pytorch
`clamp(min=0)` does the same work as ReLU:
```python
h_relu = self.linear1(x).clamp(min=0)
```
